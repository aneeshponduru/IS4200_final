{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aneeshponduru/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>sen_id</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>start time</th>\n",
       "      <th>end time</th>\n",
       "      <th>split</th>\n",
       "      <th>id</th>\n",
       "      <th>__index_level_0__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video0</td>\n",
       "      <td>a car is shown</td>\n",
       "      <td>77300</td>\n",
       "      <td>9</td>\n",
       "      <td>https://www.youtube.com/watch?v=9lZi22qLlEo</td>\n",
       "      <td>137.72</td>\n",
       "      <td>149.44</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video1</td>\n",
       "      <td>in a kitchen a woman adds different ingredient...</td>\n",
       "      <td>110460</td>\n",
       "      <td>16</td>\n",
       "      <td>https://www.youtube.com/watch?v=w4JM08PDEng</td>\n",
       "      <td>184.33</td>\n",
       "      <td>206.89</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video10</td>\n",
       "      <td>a man holds two dogs</td>\n",
       "      <td>47320</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.youtube.com/watch?v=CcJwo2eyfI0</td>\n",
       "      <td>33.33</td>\n",
       "      <td>46.53</td>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video100</td>\n",
       "      <td>a basset hound sits outside a door</td>\n",
       "      <td>18360</td>\n",
       "      <td>12</td>\n",
       "      <td>https://www.youtube.com/watch?v=6S-47swQBBU</td>\n",
       "      <td>1146.06</td>\n",
       "      <td>1156.44</td>\n",
       "      <td>train</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video1000</td>\n",
       "      <td>a woman is wearing a costume</td>\n",
       "      <td>49000</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.youtube.com/watch?v=ALrHNDBK-jw</td>\n",
       "      <td>738.93</td>\n",
       "      <td>749.93</td>\n",
       "      <td>train</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    video_id                                            caption  sen_id  \\\n",
       "0     video0                                     a car is shown   77300   \n",
       "1     video1  in a kitchen a woman adds different ingredient...  110460   \n",
       "2    video10                               a man holds two dogs   47320   \n",
       "3   video100                 a basset hound sits outside a door   18360   \n",
       "4  video1000                       a woman is wearing a costume   49000   \n",
       "\n",
       "   category                                          url  start time  \\\n",
       "0         9  https://www.youtube.com/watch?v=9lZi22qLlEo      137.72   \n",
       "1        16  https://www.youtube.com/watch?v=w4JM08PDEng      184.33   \n",
       "2         6  https://www.youtube.com/watch?v=CcJwo2eyfI0       33.33   \n",
       "3        12  https://www.youtube.com/watch?v=6S-47swQBBU     1146.06   \n",
       "4         7  https://www.youtube.com/watch?v=ALrHNDBK-jw      738.93   \n",
       "\n",
       "   end time  split    id  __index_level_0__  \n",
       "0    149.44  train     0                  0  \n",
       "1    206.89  train     1                  1  \n",
       "2     46.53  train    10                  2  \n",
       "3   1156.44  train   100                  3  \n",
       "4    749.93  train  1000                  4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MSR-VTT train parquet file\n",
    "df_raw = pd.read_parquet(\"train-00000-of-00001-60e50ff5fbbd1bb5.parquet\")\n",
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check if a video exists on YouTube\n",
    "# exists if the video is available and not private\n",
    "def video_exists_youtube(url):\n",
    "    try:\n",
    "        # Headers to mimic a browser request\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "        }\n",
    "        # Get the video page\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        time.sleep(1)  # Be polite to the server\n",
    "\n",
    "        # Check if the response is successful\n",
    "        if response.status_code != 200:\n",
    "            return False\n",
    "\n",
    "        # Get the HTML content and intialize unavailable phrases\n",
    "        html = response.text.lower()\n",
    "        unavailable_phrases = [\n",
    "            \"video is not available\",\n",
    "            \"video unavailable\",\n",
    "            \"this video is private\",\n",
    "            \"this video has been removed\",\n",
    "            \"this video is no longer available\"\n",
    "        ]\n",
    "\n",
    "        # Check if any of the unavailable phrases are in the HTML\n",
    "        for phrase in unavailable_phrases:\n",
    "            if phrase in html:\n",
    "                return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    # exception handling for request failures\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking video {url}: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkingyt\n",
      "start\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Sample 90 unique video IDs and add 1 relevant video for each query (100 total videos)\n",
    "# Drop duplicates per video\n",
    "df_unique = df_raw.drop_duplicates(subset=[\"video_id\"])\n",
    "\n",
    "# Check availability for first 200 (this is for B-grade implementation. More videos will be added later)\n",
    "urls = df_unique[\"url\"].tolist()[:230]\n",
    "\n",
    "# Check which videos are live\n",
    "valid_urls = [url for url in urls if video_exists_youtube(url)][:200]\n",
    "\n",
    "# Sample 90 from valid video IDs\n",
    "df_valid = df_unique[df_unique[\"url\"].isin(valid_urls)]\n",
    "sampled_video_ids = df_valid.sample(n=90, random_state=1)[\"video_id\"].to_list()\n",
    "\n",
    "# Add 10 handpicked relevant videos\n",
    "relevant_video_ids = [\n",
    "    \"video5870\",\n",
    "    \"video4900\",\n",
    "    \"video5524\",\n",
    "    \"video5830\",\n",
    "    \"video3855\",\n",
    "    \"video3604\",\n",
    "    \"video5942\",\n",
    "    \"video3525\",\n",
    "    \"video1634\",\n",
    "    \"video453\"\n",
    "]\n",
    "\n",
    "# Filter to unique video-caption pairs\n",
    "df_unique = df_raw.drop_duplicates(subset=[\"video_id\"])\n",
    "\n",
    "# Get full rows for the 90 random videos\n",
    "df_sampled_90 = df_unique[df_unique[\"video_id\"].isin(sampled_video_ids)]\n",
    "\n",
    "# Get full rows for the 10 handpicked relevant videos\n",
    "df_manual_10 = df_unique[df_unique[\"video_id\"].isin(relevant_video_ids)]\n",
    "\n",
    "# Combine to get one dataframe with 100 unique video IDs and captions\n",
    "sampled_video_ids = pd.concat([df_sampled_90, df_manual_10], ignore_index=True)[[\"video_id\", \"caption\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for all data for the sampled videos\n",
    "df_sample = df_raw[df_raw[\"video_id\"].isin(sampled_video_ids['video_id'].to_list())]\n",
    "df_metadata = df_sample.groupby(\"video_id\").first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions for BM25\n",
    "df_metadata[\"tokens\"] = df_metadata[\"caption\"].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "# Initialize BM25 with the tokenized captions\n",
    "bm25 = BM25Okapi(df_metadata[\"tokens\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list of queries\n",
    "queries = [\n",
    "    \"a snowy mountain with a clear blue sky\",\n",
    "    \"a crowded city street at night with neon signs\",\n",
    "    \"a peaceful beach with palm trees and waves crashing\",\n",
    "    \"a person jogging through a park in the morning\",\n",
    "    \"a chef chopping vegetables in a kitchen\",\n",
    "    \"a surfer riding a big wave\",\n",
    "    \"a red sports car in front of a building\",\n",
    "    \"a golden retriever sitting on a grassy field\",\n",
    "    \"a firefighter rescuing a person from a burning building at night\",\n",
    "    \"a person near luxury cars in cloudy weather speaking to someone\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Iterate through each query and get the top 5 results\n",
    "for query in queries:\n",
    "    # Tokenize the query\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "    # Get BM25 scores for the query\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    # Get the indices of the top 5 scores\n",
    "    top_indices = np.argsort(scores)[-5:][::-1]\n",
    "    \n",
    "    # For each of the top 5 results, append the necessary details to the results list\n",
    "    for rank, idx in enumerate(top_indices):\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": rank + 1,\n",
    "            \"video_id\": df_metadata.iloc[idx][\"video_id\"],\n",
    "            \"caption\": df_metadata.iloc[idx][\"caption\"],\n",
    "            \"score\": scores[idx],\n",
    "            \"link\": df_metadata.iloc[idx][\"url\"],\n",
    "            \"start\": df_metadata.iloc[idx][\"start time\"],\n",
    "            \"end\": df_metadata.iloc[idx][\"end time\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 retrieval results saved to bm25_b_level_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV for manual annotation\n",
    "results_df = pd.DataFrame(results)\n",
    "if not os.path.exists(\"bm25_b_level_results.csv\"):\n",
    "    results_df.to_csv(\"bm25_b_level_results.csv\", index=False)\n",
    "    print(\"Results saved to bm25_b_level_results.csv\")\n",
    "else:\n",
    "    print(\"File already exists. No save performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG@5 per query:\n",
      "- a chef chopping vegetables in a kitchen...: 1.000\n",
      "- a crowded city street at night with neon signs...: 0.924\n",
      "- a firefighter rescuing a person from a burning bui...: 0.877\n",
      "- a golden retriever sitting on a grassy field...: 0.920\n",
      "- a peaceful beach with palm trees and waves crashin...: 1.000\n",
      "- a person jogging through a park in the morning...: 0.712\n",
      "- a person near luxury cars in cloudy weather speaki...: 0.865\n",
      "- a red sports car in front of a building...: 0.724\n",
      "- a snowy mountain with a clear blue sky...: 0.924\n",
      "- a surfer riding a big wave...: 0.000\n",
      "\n",
      "Mean NDCG@5 across all queries: 0.795\n"
     ]
    }
   ],
   "source": [
    "# Reload the results after manual annotation\n",
    "df = pd.read_csv(\"bm25_b_level_results.csv\")\n",
    "\n",
    "# Convert the relevance column to numeric, handling errors and filling NaN with 0\n",
    "df[\"relevance\"] = pd.to_numeric(df[\"relevance\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# Function to calculate DCG\n",
    "def dcg(scores):\n",
    "    return sum(score / np.log2(idx + 2) for idx, score in enumerate(scores))\n",
    "\n",
    "# Dictionary to store NDCG scores\n",
    "ndcg_scores = {}\n",
    "\n",
    "# Calculate NDCG for each query\n",
    "for query, group in df.groupby(\"query\"):\n",
    "    # Sort by predicted rank \n",
    "    top_k = group.sort_values(\"rank\").head(5)\n",
    "    \n",
    "    # Calculate DCG for the ranking\n",
    "    relevance = top_k[\"relevance\"].tolist()\n",
    "    actual_dcg = dcg(relevance)\n",
    "    \n",
    "    # Ideal DCG (perfect ranking of same scores)\n",
    "    ideal_relevance = sorted(relevance, reverse=True)\n",
    "    ideal_dcg = dcg(ideal_relevance)\n",
    "    \n",
    "    # Calculate NDCG\n",
    "    ndcg = actual_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "    ndcg_scores[query] = ndcg\n",
    "\n",
    "# Print per-query and average NDCG@5\n",
    "print(\"NDCG@5 per query:\")\n",
    "for query, score in ndcg_scores.items():\n",
    "    print(f\"- {query[:50]}...: {score:.3f}\")\n",
    "\n",
    "mean_ndcg = np.mean(list(ndcg_scores.values()))\n",
    "print(f\"\\nMean NDCG@5 across all queries: {mean_ndcg:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sampled data for the 100 videos\n",
    "#df_raw[df_raw['video_id'].isin(sampled_video_ids['video_id'].to_list())].to_csv(\"sampled_data_100_videos.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
